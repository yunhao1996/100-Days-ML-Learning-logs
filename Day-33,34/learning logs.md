# 随机森林(RandomForests)学习笔记
## 概念
```
随机森林是有监督的集成学习模型（ensemble-learning model）,用于分类和回归。由于决策树每一步考虑的是局部最优，所以容易过拟  
合，随机森林采用多个决策树的投票机制来改善决策树，以获得更准确和稳定的预测。
集成学习模型是复合了多个机器学习模型，多个模型个体集合在一起的时候，整体的功能会变得强大。
```
## 工作原理
```
随机深林算法分为两步，第一步是创建决策树，第二步是根据第一步中的决策树的分类器结果做出预测。相较于决策树算法，在随机森林中，  
查找根节点和分割特征节点的过程是随机进行的。  
A.针对于构建，采用Bagging(套袋法)
每棵树按如下方式生成：
1.(数据的随机选取)如果训练集中有N个样本，有放回的随机抽取n个。这个样本将是生成该决策树的训练集。  
2.（特征的随机选取）对于每个样本，如果有M个输入变量（或特征），指定一个常数m，然后随机地从M个特征中  选取m个特征子集。然后  将m个特征中的最优的分裂特征用来分裂节点。(ps:传统的决策树在选择划分属性时是在当前节点的属性集合中选择一个最优属性，假设有d个属性；而在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：若令k=d,则基决策树的构建与传统决策树相同；若令k=1,则是随机选择一个属性用于划分；一般情况下，推荐值k=log(d))。
最后，重复多次，产生多棵决策树
B.在第二步预测，随机森林分为三步  
1.使用每一个随机创建的决策树的规则来预测测试特征的结果（目标）  
2.计算每个预测目标的票数  
3.获得票数最高的预测目标视为随机森林算法的最终预测。
```
## RandomForestClassifier参数说明
```
n_estimators: RF中决策树的数目，默认为10  
criterion:指定分裂的标准，默认标准为gini(基尼不纯度)，此外，还包括entropy(信息增益的熵)  
max_features:查找最佳分裂所考虑的特征数，默认为auto  
max_death:树的最大深度，默认为None时，那么会扩展节点，直到所有的叶子是纯净的，这句话我的理解就是应用所有的特征构造决策树，不限制树的深度  
min_samples_leaf:最小叶子节点样本数，默认为1  
min_samples_split:最小分裂样本数量，默认值为2  
min_weight_fraction_leaf:最小叶子节点权重 ，默认为0  
max_leaf_nodes:最大叶子节点数，默认为None时，不限制叶子节点的数量  
min_impurity_split:分裂的最小不纯度相当于阈值，如果一个节点的不纯度超过阈值那么这个节点将会分裂，否则它还是一片叶子  
bootstrap；建立决策树时，是否使用有放回抽样，默认为True.为什么要有放回？个人理解：如果不放回，每棵决策树的训练样本集都是不同的，没有交集，这样，树之间可能存在很大的差异，不利于后面的投票表决  
oob_score:是否使用袋外样本估计准确度，默认关闭  
n_jobs:并行job数，默认为1，为-1时代表全部  
random_state:随机数种子，默认为None时，每次运行结果都是不同的，为具体的整数时，如0，1，2等，相当于一种规则，保证每次运行结果相同  
verbose:控制决策树建立过程的冗余度（重复的部分或者特征），默认为0  
warm_start:如果设置为True，在之前的模型基础上预测并添加模型，否则，建立一个全新的森林,默认为False  
class_weight:“balanced” 模式自动调整权重
```
