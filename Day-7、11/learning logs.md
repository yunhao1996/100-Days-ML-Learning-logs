# learning logs_ML_Day7/11
* 学习K-NN算法程序盲点：
1.
```python
import numpy as np
dataset = [[1, 2], [2, 3], [3, 4]]
print(dataset)
a = np.array([[1, 2], [2, 3], [3, 4]])
print(a)
a1 = np.mat([[1, 2], [2, 3], [3, 4]])
print(a1)
a2 = a1.tolist()
print(a2)
```
数组，列表，矩阵之间的转换，`dataset`为列表，`a`为数组，`a1·`为矩阵。很多时候，`matrix`和`array`是通用的（一维时有区别），在*python*程序中，`array`更常用。`matrix`的优势在于矩阵运算。`.tolist`是转列表命令，不在`numpy`库中。  
2.*numpy*库中的`tile`函数：
```python
import numpy as np
a1 = np.tile([0, 0], 5)  # 在列的方向重复[0,0] 5次
print(a1)
a2 = np.tile([1, 2], (3, 4))  # 行的方向重复2次，列方向重复4次
print(a2)
```
3.`**`:乘方或开方运算  
4.`np.sort()`: *sort*意为将...排序，用于数组排序；`np.argsort()`:数组排序，返回其下标（序号）。  
```python
import numpy as np
a=np.mat([[4,2],[3,4]])
b=np.argsort(a)
c=np.sort(a)
print(b)
print(c)
```
输出：
```python
[[1 0]
 [0 1]]
[[2 4]
 [3 4]]
```
## K-NN

* 监督学习：从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。  

  无监督学习：输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，

  类间差距最大化。  

* K-NN :K-NearestNeighbor,K最近邻。  

* 算法流程：  
           1. 准备数据，对数据进行预处理
           2. 选用合适的数据结构存储训练数据和测试元组  
           3. 设定参数，如k  
           4. 维护一个大小为k的的按距离由大到小的优先级队列，用于存储最近邻训练元组。随机从训练元组中选取k个元组作为初始的最近邻元组，分别计算测试元               组到这k个元组的距离，将训练元组标号和距离存入优先级队列  
           5. 遍历训练元组集，计算当前训练元组与测试元组的距离，将所得距离L 与优先级队列中的最大距离Lmax  
           6. 进行比较。若L>=Lmax，则舍弃该元组，遍历下一个元组。若L < Lmax，删除优先级队列中最大距离的元组，将当前训练元组存入优先级队列。  
           7. 遍历完毕，计算优先级队列中k 个元组的多数类，并将其作为测试元组的类别。  
           8. 测试元组集测试完毕后计算误差率，继续设定不同的k值重新进行训练，最后取误差率最小的k 值  
 * 理解：
 
 <p align="center">
   <img src="https://github.com/yunhao1996/100-Days-ML-Learning-logs/blob/master/Day-7、11/knn.png">
 </p>
   如果选取3个最近的邻居（K=3），红色三角形占两个，蓝色正方形占一个，所以将绿色圆圈视为红色三角形类。 如果选取5个最近的邻居（K=3），红色三角形占两个，蓝色正方形占三个，所以将绿色圆圈视为蓝色正方形类。
    所以，k值的选择很重要K值设置过小会降低分类精度；若设置过大，且测试样本属于训练集中包含数据较少的类，则会增加噪声，降低分类效果。
通常，K值的设定采用交叉检验的方式（以K=1为基准）。网上经验规则：K一般低于训练样本数的平方根。
  
